{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 변수 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "\n",
    "import bareunpy\n",
    "from bareunpy import Tagger\n",
    "from jamo import h2j, j2hcj\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize \n",
    "from konlpy.tag import Komoran\n",
    "komoran = Komoran()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 필요한 데이터 불러오기\n",
    "\n",
    "# 전처리된 신문 기사 텍스트\n",
    "data = pd.read_csv('변수포함 데이터셋_6차_9520.csv', encoding = 'utf-8-sig')\n",
    "\n",
    "# 모두의 말뭉치 일상 빈도\n",
    "corp_freq = pd.read_csv('corpus_frequency.csv', encoding = 'utf-8-sig')\n",
    "corp_freq.drop(['Unnamed: 0'], inplace = True, axis = 1)\n",
    "\n",
    "# 물결 21 빈도\n",
    "wave_corp = pd.read_csv('물결21_총빈도수_최종.csv', encoding='utf-8-sig')\n",
    "wave_corp.drop(['Unnamed: 0.1', 'Unnamed: 0'], inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 일상에서 자주 쓰이는지를 확인하기 위한 변수이기 때문에 quantile 확인 후, 빈도수로 잘라야함\n",
    "q1 = np.percentile(corp_freq['frequency'], 25)\n",
    "median = np.median(corp_freq['frequency'])\n",
    "q3 = np.percentile(corp_freq['frequency'], 75)\n",
    "mean = np.mean(corp_freq['frequency'])\n",
    "variance = np.var(corp_freq['frequency'])\n",
    "std_dev = np.std(corp_freq['frequency'])\n",
    "print('1사분위 수 : {}, 중위값 : {}, 3사분위 수 : {}, 평균 {}, 분산 {}, 표준편차 : {}'.format(q1, median, q3, mean, variance, std_dev))\n",
    "\n",
    "# 평균 이상으로 일상 빈도 업데이트\n",
    "daily_usage = corp_freq[corp_freq['frequency'] >= mean]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) 음운론적 8단계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexityAnalyzer:\n",
    "    \n",
    "    def __init__(self, already_know=[]):\n",
    "        self.already_know = already_know # 학습단어 연동\n",
    "        self.CHO_basic = ['ㄱ', 'ㄴ', 'ㄷ', 'ㄹ', 'ㅁ', 'ㅂ', 'ㅅ', 'ㅇ', 'ㅈ', 'ㅎ'] \n",
    "        self.CHO_advanced = ['ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㄲ', 'ㄸ', 'ㅃ', 'ㅆ', 'ㅉ'] \n",
    "        self.JOONG_single = ['ㅏ', 'ㅓ', 'ㅗ', 'ㅜ', 'ㅐ', 'ㅔ', 'ㅡ', 'ㅣ'] \n",
    "        self.JOONG_double = ['ㅑ', 'ㅕ', 'ㅛ', 'ㅠ', 'ㅒ', 'ㅖ', 'ㅘ', 'ㅙ', 'ㅚ', 'ㅞ', 'ㅝ', 'ㅟ', 'ㅢ'] \n",
    "        self.JONG = ['ㄱ', 'ㄲ', 'ㄳ', 'ㄴ', 'ㄵ', 'ㄶ', 'ㄷ', 'ㄹ', 'ㄺ', 'ㄻ', 'ㄼ', 'ㄽ', 'ㄾ', 'ㄿ', 'ㅀ', 'ㅁ', 'ㅂ', 'ㅄ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']\n",
    "\n",
    "        self.weight_list = [] # 가중치 담을 리스트 준비\n",
    "        self.level_weight = None # 가중치 초기화\n",
    "\n",
    "        self.already_words = [] # 문장 내 학습 단어 담을 리스트 준비\n",
    "        \n",
    "        \n",
    "    def syllable_tokenizer(self, s):\n",
    "        temp = s.replace(' ', '').replace('\\n', '').replace('\"', '')\n",
    "        result = []\n",
    "        for c in temp:\n",
    "            result.append(c)\n",
    "        return result\n",
    "    \n",
    "    def normalizer(self, value): # 가중치 때문에 최저 0.5까지 나올 수 있음!\n",
    "        return (value - 0.5) / (8 - 1)\n",
    "    \n",
    "    def text_level(self, text):\n",
    "        # 음절단위 분리\n",
    "        text_syl = self.syllable_tokenizer(text)\n",
    "        \n",
    "        # 음소단위 분리\n",
    "        pho_list = []\n",
    "        for syl in text_syl:\n",
    "            phoneme = j2hcj(h2j(syl))\n",
    "            pho_list.append(phoneme)\n",
    "        \n",
    "        # complexity 판별\n",
    "        level_list = []\n",
    "        \n",
    "        def jong_no(word):\n",
    "            if word[0] in self.CHO_basic and word[1] in self.JOONG_single:\n",
    "                level = 1\n",
    "                level_list.append(level)\n",
    "            elif word[0] in self.CHO_advanced and word[1] in self.JOONG_single:\n",
    "                level = 2\n",
    "                level_list.append(level)\n",
    "            elif word[0] in self.CHO_basic and word[1] in self.JOONG_double:\n",
    "                level = 5\n",
    "                level_list.append(level)\n",
    "            elif word[0] in self.CHO_advanced and word[1] in self.JOONG_double:\n",
    "                level = 7\n",
    "                level_list.append(level)\n",
    "            else :\n",
    "                level = np.NAN\n",
    "                level_list.append(level)\n",
    "        \n",
    "        def jong_yes(word):\n",
    "            if word[0] in self.CHO_basic and word[1] in self.JOONG_single and word[2] in self.JONG:\n",
    "                level = 3\n",
    "                level_list.append(level)\n",
    "            elif word[0] in self.CHO_advanced and word[1] in self.JOONG_single and word[2] in self.JONG:\n",
    "                level = 4\n",
    "                level_list.append(level)\n",
    "            elif word[0] in self.CHO_basic and word[1] in self.JOONG_double and word[2] in self.JONG:\n",
    "                level = 6\n",
    "                level_list.append(level)\n",
    "            elif word[0] in self.CHO_advanced and word[1] in self.JOONG_double and word[2] in self.JONG:\n",
    "                level = 8\n",
    "                level_list.append(level)\n",
    "            else :\n",
    "                level = np.NAN\n",
    "                level_list.append(level)\n",
    "        \n",
    "        for pho in pho_list:\n",
    "            if len(pho) == 2:\n",
    "                jong_no(pho)\n",
    "            elif len(pho) == 3:\n",
    "                jong_yes(pho)\n",
    "                \n",
    "        ## [NEW] 가중치 반영\n",
    "        if self.level_weight is not None : # 가중치 리스트 있는 경우에만!\n",
    "            #print('level weight', self.level_weight) # 가중치 확인하고 싶으면 '#'을 해제하세요\n",
    "            level_list = [a * b for a, b in zip(level_list, self.level_weight)] # 레벨리스트에 가중치리스트 곱해서 최종 레벨리스트 생성!\n",
    "        else :\n",
    "            #print(\"level list:\", level_list) # 난이도 확인하고 싶으면 '#'을 해제하세요\n",
    "            pass\n",
    "        \n",
    "        \n",
    "        if len(level_list) == 0:\n",
    "            #print(text, \"---> 난이도 불필요 문장\") # 난이도 불필요 문장을 확인하고 싶으면 '#'을 해제하세요\n",
    "            return 0, level_list # 잘못된 경우 0(zero)을 반환 (NaN 값 처리)\n",
    "        \n",
    "        syl_len = len(level_list)  # 음절 수\n",
    "        level_sum = sum(level_list)  # complexity 총합\n",
    "        score = level_sum / syl_len\n",
    "        \n",
    "        score_scaled = self.normalizer(score)  # 스케일링\n",
    "        score_per = round(score_scaled * 100, 1)  # 점수화\n",
    "        \n",
    "        # 어려운 단어 추출에 활용하기 위해 level_list도 함께 반환!\n",
    "        return score_per, level_list\n",
    "    \n",
    "    def analyze_syl(self, text): ##음절단위 분석기##\n",
    "        sent_2 = text\n",
    "        score_list1 = []\n",
    "        weight_list = None # 가중치리스트 초기화\n",
    "        \n",
    "        score_t = self.text_level(sent_2)\n",
    "\n",
    "        if score_t == 0 :\n",
    "            score_t = 0\n",
    "            score_list1.append(score_t)\n",
    "        else:\n",
    "            score_t = score_t[0] # score 값만 사용할 거라서 인덱싱\n",
    "            score_list1.append(score_t)\n",
    "                \n",
    "        \n",
    "        df1 = pd.DataFrame({'문장 내용': sent_2, '음절단위 점수': score_list1})\n",
    "        return df1\n",
    "    \n",
    "    def analyze_noun(self, text): ##명사단위 분석기##\n",
    "        sent_2 = text\n",
    "        score_list2 = []\n",
    "        difficult_list = []  # 어려운 음절 저장할 리스트\n",
    "        \n",
    "        # 명사만 뽑아낸 뒤에,,,\n",
    "        sent_2_noun = []\n",
    "        for t1 in sent_2 :\n",
    "            nouns_2 = komoran.nouns(t1)\n",
    "            sent_2_noun.append(nouns_2)\n",
    "            \n",
    "        # ,,, 다시 입력 형태에 맞게 문자열로 이어붙이기!  \n",
    "        noun_2_str = []\n",
    "        for t2 in sent_2_noun : \n",
    "            \n",
    "            weights, words = self.weight_maker(t2) # 가중치 만드는 함수를 거치는 구간 \n",
    "            self.weight_list.append(weights)\n",
    "            self.already_words.append(words)\n",
    "            \n",
    "            comb_nouns = ' '.join(t2) # 문자열 이어붙이는 구간\n",
    "            noun_2_str.append(comb_nouns)\n",
    "        \n",
    "        # 그리고 그걸 다시 text_level에 입력!\n",
    "        score_list2 = []\n",
    "\n",
    "        for idx, t3 in enumerate(noun_2_str) : \n",
    "            self.level_weight = self.weight_list[idx] # 해당 문장에 맞는 가중치 리스트 꺼내기\n",
    "            score_t, levels = self.text_level(t3)\n",
    "            if score_t == 0 :\n",
    "                score_t = 0\n",
    "                score_list2.append(score_t)\n",
    "            else:\n",
    "                \n",
    "                score_list2.append(score_t)\n",
    "                \n",
    "                 # [NEW] 어려운 단어 추출\n",
    "                \n",
    "            diff_syls = []            \n",
    "            for ldx, level in enumerate(levels):\n",
    "                t3 = t3.replace(' ', '').replace('\\n', '').replace('\"', '') # 길이 맞추기 위해 동일하게 전처리해주고\n",
    "                if level >= 5:\n",
    "                    diff_t3 = \"[{}]{}\".format(ldx, t3[ldx])\n",
    "                    diff_syls.append(diff_t3)\n",
    "                else:\n",
    "                    diff_syls.append(' ')\n",
    "            \n",
    "            diff_syls = list(dict.fromkeys(diff_syls)) # 중복 제거 (set은 순서가 뒤섞여서 사용 취소)\n",
    "            diff_syls = '  '.join(diff_syls) # 보기 좋게 대괄호 제거\n",
    "                    \n",
    "            difficult_list.append(diff_syls) # 문장 돌면서 '문장별 어려운 음절' 추가\n",
    "            \n",
    "        already_words_set = [', '.join(lst) for lst in self.already_words] # 학습한 단어들 출력 (깔끔하게 대괄호 제거)\n",
    "        \n",
    "        \n",
    "        df2 = pd.DataFrame({'문장 내용': sent_2, '명사단위 점수': score_list2, '난이도 5 이상 음절': difficult_list, '학습한 단어': already_words_set})\n",
    "        return df2\n",
    "\n",
    "    def weight_maker(self, nounset) : \n",
    "        weight_already = []\n",
    "        word_already = []\n",
    "        \n",
    "        for noun in nounset : \n",
    "\n",
    "            if noun in self.already_know : # 만약 이미 학습한 단어라면,\n",
    "                word_already.append(noun) # (ㄱ) 학습한 단어 목록에 추가\n",
    "                for i in range(len(noun)) : # (ㄴ) 가중치 목록에 음절 수만큼 가중치 추가\n",
    "                    weight_already.append(0.5)\n",
    "            else :\n",
    "                for i in range(len(noun)) :\n",
    "                    weight_already.append(1)\n",
    "        \n",
    "        return weight_already, word_already # 생성된 가중치리스트를 반환해주자!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHO_basic = ['ㄱ', 'ㄴ', 'ㄷ', 'ㄹ', 'ㅁ', 'ㅂ', 'ㅅ', 'ㅇ', 'ㅈ', 'ㅎ'] # 기본자음 10개\n",
    "CHO_advanced = ['ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㄲ', 'ㄸ', 'ㅃ', 'ㅆ', 'ㅉ'] # 격음 또는 경음 9개\n",
    "\n",
    "JOONG_single = ['ㅏ', 'ㅓ', 'ㅗ', 'ㅜ', 'ㅐ', 'ㅔ', 'ㅡ', 'ㅣ'] # 단모음 8개\n",
    "JOONG_double = ['ㅑ', 'ㅕ', 'ㅛ', 'ㅠ', 'ㅒ', 'ㅖ', 'ㅘ', 'ㅙ', 'ㅚ', 'ㅞ', 'ㅝ', 'ㅟ', 'ㅢ'] # 이중모음 13개\n",
    "\n",
    "JONG = ['ㄱ', 'ㄲ', 'ㄳ', 'ㄴ', 'ㄵ', 'ㄶ', 'ㄷ', 'ㄹ', 'ㄺ', 'ㄻ', 'ㄼ', 'ㄽ', 'ㄾ', 'ㄿ', 'ㅀ', 'ㅁ', 'ㅂ', 'ㅄ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']\n",
    "\n",
    "\n",
    "def syllable_tokenizer(s):\n",
    "    temp = s.replace(' ', '').replace('\\n', '').replace('\"', '') # 불필요한 기호 및 줄바꿈 삭제\n",
    "    result = []\n",
    "    for c in temp: # 입력받은 temp를 인덱싱으로 돌면서\n",
    "        result.append(c) # 한 음절씩 결과 리스트에 추가\n",
    "    return result\n",
    "\n",
    "def normalizer(value) : # score값들을 정규화해주는 함수를 정의\n",
    "    return (value - 1) / (8 - 1) # 최소1 최대8의 값을 0~1 사이로 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_level(text) :\n",
    "    \n",
    "     # (1) 음절단위 분리\n",
    "    text_syl = syllable_tokenizer(text)\n",
    "    \n",
    "    # (2) 음소단위 분리\n",
    "    pho_list=[]\n",
    "    for syl in text_syl :\n",
    "        phoneme = j2hcj(h2j(syl))\n",
    "        pho_list.append(phoneme)\n",
    "    \n",
    "    # (3) 복잡도 판별\n",
    "    level_list = []\n",
    "    # 함수 설정 (깔끔한 출력을 위해 불필요한 print문 전부 제거함)\n",
    "    def jong_no(word):  \n",
    "        if word[0] in CHO_basic and word[1] in JOONG_single :\n",
    "            level = 1\n",
    "            level_list.append(level)\n",
    "        elif word[0] in CHO_advanced and word[1] in JOONG_single :\n",
    "            level = 4\n",
    "            level_list.append(level)\n",
    "        elif word[0] in CHO_basic and word[1] in JOONG_double :\n",
    "            level = 25\n",
    "            level_list.append(level)\n",
    "        elif word[0] in CHO_advanced and word[1] in JOONG_double :\n",
    "            level = 49\n",
    "            level_list.append(level)\n",
    "        else :\n",
    "            pass\n",
    "        \n",
    "    def jong_yes(word):\n",
    "        if word[0] in CHO_basic and word[1] in JOONG_single and word[2] in JONG :\n",
    "            level = 9\n",
    "            level_list.append(level)\n",
    "        elif word[0] in CHO_advanced and word[1] in JOONG_single and word[2] in JONG :\n",
    "            level = 16\n",
    "            level_list.append(level)\n",
    "        elif word[0] in CHO_basic and word[1] in JOONG_double and word[2] in JONG :\n",
    "            level = 36\n",
    "            level_list.append(level)\n",
    "        elif word[0] in CHO_advanced and word[1] in JOONG_double and word[2] in JONG :\n",
    "            level = 64\n",
    "            level_list.append(level)\n",
    "        else :\n",
    "            pass\n",
    "    \n",
    "    for pho in pho_list : # (2)의 결과 pho_list에 대해서\n",
    "        if len(pho) == 2 : # 받침이 없는 경우는 jong_no 함수로\n",
    "            jong_no(pho)\n",
    "        elif len(pho) == 3 : # 받침이 있는 경우는 jong_yes 함수로\n",
    "            jong_yes(pho)\n",
    "            \n",
    "    #print(level_list) 레벨 리스트 완성\n",
    "    \n",
    "    # 문장이 아예 문장부호로만 이루어진 경우 등 level_list에 아무것도 없는 경우는 제외가 필요함\n",
    "    if len(level_list)==0 :\n",
    "        print(text, \"---> 난이도 불필요 문장\")\n",
    "        pass\n",
    "    \n",
    "    else :      \n",
    "        # (4) 평균복잡도 산출\n",
    "        syl_len = len(level_list) # 음절 수 \n",
    "        #print(syl_len)\n",
    "        level_sum = sum(level_list) # 복잡도 총합\n",
    "        #print(level_sum)\n",
    "        score = level_sum / syl_len\n",
    "\n",
    "        # (5) 스케일링 및 점수화\n",
    "        score_scaled = normalizer(score) # normalizer 함수에 score 입력\n",
    "        score_per = round(score_scaled*100, 1) # 100을 곱해서 점수화\n",
    "\n",
    "        # (6) 정규화 안 하고 그냥 출력!\n",
    "        return score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋에 적용\n",
    "data['음운론적 복잡도'] = data['본문'].apply(text_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) 바른 형태소 분석기를 사용한 변수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tagger 정의\n",
    "API_KEY=\"[바른 형태소 분석기 api key]\" \n",
    "my_tagger = Tagger(API_KEY, 'localhost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 데이터 불러오기\n",
    "\n",
    "# 취합본 데이터\n",
    "data = pd.read_csv('~~취합본 데이터~~', encoding='utf-8-sig')\n",
    "\n",
    "# 일상빈도 데이터\n",
    "daily_corp = pd.read_csv('~~일상빈도 데이터~~', encoding='utf-8-sig')\n",
    "\n",
    "# 물결21 데이터\n",
    "wave_corp = pd.read_csv('~~물결21 데이터~~', encoding='utf-8-sig')\n",
    "\n",
    "# 기초어휘 데이터\n",
    "basic_corp = pd.read_csv('~~기초어휘 데이터~~', encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 취합본 데이터에서 필요한 부분만 추출\n",
    "data = data[['언론사', '본문']]\n",
    "\n",
    "# 일상빈도 데이터에서 필요한 부분만 추출\n",
    "daily_corp = daily_corp[daily_corp['frequency'] >= np.mean(daily_corp['frequency'])]\n",
    "\n",
    "# 물결21 데이터에서 필요한 부분만 추출\n",
    "wave_corp = wave_corp[['태그', '단어', '총 빈도수']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 품사 태그 지정 -> 수식언, 연결어미, 전성어미\n",
    "susik_set = ['MMA', 'MAG', 'MAJ']\n",
    "junsung_set = ['ETN', 'ETM']\n",
    "yeongyeol_set = 'EC'\n",
    "important_pos_list = [\"NNG\", 'NNP', \"NNB\", \"NP\", \"NR\", \"NF\", \"NA\", \"NV\", \"VV\", \"VA\", \"VX\", \"VCP\", \"VCN\", \"MMA\", \"MMD\", \"MMN\", \"MAG\", \"MAJ\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시작 시간 기록\n",
    "start_time = time.time()\n",
    "\n",
    "# 값을 저장할 리스트\n",
    "average_usage_list = []\n",
    "count_list = []\n",
    "susik_list = []\n",
    "junsung_list = []\n",
    "yeongyeol_list = []\n",
    "s_length_list = []\n",
    "wave_list = []\n",
    "basic_list = []\n",
    "\n",
    "# '본문'에 대한 처리 시간과 진행 상황을 모니터링\n",
    "for i, line in enumerate(data['본문'], start=1):\n",
    "    res = my_tagger.tags([line])\n",
    "    pa = res.pos() # 일상 빈도\n",
    "    ma = res.morphs() # 물결 21\n",
    "    \n",
    "    # 문장 개수 확인을 위한 tagger(auto_split의 유무가 영향을 주는가?)\n",
    "    res = my_tagger.tags([line], auto_split=True) # 문장 개수\n",
    "    m = res.msg()\n",
    "\n",
    "    # 'pa' 리스트에 있는 형태소의 총 빈도수 합산\n",
    "    total_usage_pa = 0\n",
    "\n",
    "    for word, _ in pa:\n",
    "        if word in daily_corp['corpus'].values:\n",
    "            frequency = daily_corp.loc[daily_corp['corpus'] == word, 'frequency'].sum()\n",
    "            total_usage_pa += frequency\n",
    "            \n",
    "    # 기초 어휘 count를 위한 'pa' 리스트에 있는 형태소의 총 빈도수 합산\n",
    "    total_usage_basic = 0\n",
    "\n",
    "    for word, pos in pa:\n",
    "        if pos in important_pos_list:\n",
    "            if word in basic_corp['어휘'].values:\n",
    "                match_vocab = basic_corp[basic_corp['어휘'] == word]\n",
    "                if len(match_vocab) == 1:\n",
    "                    rank = sum(match_vocab[\"등급\"].values)\n",
    "                else:\n",
    "                    rank = np.mean(match_vocab[\"등급\"].tolist())\n",
    "            else:\n",
    "                rank = 4\n",
    "        else:\n",
    "            rank = 0\n",
    "        total_usage_basic += rank\n",
    "    \n",
    "    # 기초어휘점수를 리스트에 추가\n",
    "    basic_list.append(total_usage_basic)\n",
    "            \n",
    "    # 'ma' 리스트에 있는 형태소의 총 빈도수 합산\n",
    "    total_usage_ma = 0\n",
    "\n",
    "    for morpheme in ma:\n",
    "        if morpheme in wave_corp['단어'].values:\n",
    "            frequency = wave_corp.loc[wave_corp['단어'] == morpheme, '총 빈도수'].sum()\n",
    "            total_usage_ma += frequency\n",
    "\n",
    "    # 중복을 포함한 형태소의 개수로 나누어 평균 일상 빈도수 계산\n",
    "    word_count = len(pa)\n",
    "    daily_average_usage = total_usage_pa / word_count\n",
    "\n",
    "    # 계산된 일상 빈도를 리스트에 추가\n",
    "    average_usage_list.append(daily_average_usage)\n",
    "    \n",
    "    # 중복을 포함한 형태소의 개수로 나누어 평균 물결21 빈도수 계산\n",
    "    morpheme_count = len(ma)\n",
    "    wave_average_usage = total_usage_ma / morpheme_count\n",
    "\n",
    "    # 계산된 물결21 빈도수를 리스트에 추가\n",
    "    wave_list.append(wave_average_usage)\n",
    "\n",
    "    # 형태소 개수를 리스트에 추가\n",
    "    count_list.append(word_count)\n",
    "    \n",
    "    # 수식언 개수\n",
    "    filtered_susik = [word for word, pos in pa if pos in susik_set]\n",
    "    count_susik = len(filtered_susik)\n",
    "    # 수식언 개수를 리스트에 추가\n",
    "    susik_list.append(count_susik)\n",
    "    \n",
    "    # 전성어미 개수\n",
    "    filtered_junsung = [word for word, pos in pa if pos in junsung_set]\n",
    "    count_junsung = len(filtered_junsung)\n",
    "    # 전성어미 개수를 리스트에 추가\n",
    "    junsung_list.append(count_junsung)\n",
    "    \n",
    "    # 연결어미 개수\n",
    "    filtered_yeongyeol = [word for word, pos in pa if pos == yeongyeol_set]\n",
    "    count_yeongyeol = len(filtered_yeongyeol)\n",
    "    # 연결어미 개수를 리스트에 추가\n",
    "    yeongyeol_list.append(count_yeongyeol)\n",
    "    \n",
    "    # 문장 개수를 count 후 리스트에 추가\n",
    "    length = len(m.sentences)\n",
    "    s_length_list.append(length)\n",
    "    \n",
    "    \n",
    "    # i가 10에 도달할 때마다 완료 메시지와 경과 시간 출력\n",
    "    if i % 10 == 0:\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Processed {i} samples. Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# 전체 실행 시간 출력\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Total elapsed time: {total_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터에 추가\n",
    "data['바른 문장 개수'] = s_length_list\n",
    "data['바른 형태소 개수'] = count_list\n",
    "\n",
    "data['일상사용빈도'] = average_usage_list\n",
    "data['물결21'] = wave_list\n",
    "data['바른 수식언 개수'] = susik_list\n",
    "data['바른 전성어미 개수'] = junsung_list\n",
    "data['바른 연결어미 개수'] = yeongyeol_list\n",
    "data['기초어휘난이도'] = basic_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) 음절 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 음절 개수 count하는 함수\n",
    "def count_characters(text):\n",
    "    cleaned_text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    words = cleaned_text.split()\n",
    "    total_character_count = sum(len(word) for word in words)\n",
    "    return total_character_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터에 적용\n",
    "data['음절 개수'] = data['본문'].apply(count_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 개수 세기\n",
    "def count_words(text):\n",
    "    words = text.split()\n",
    "    num_words = len(words)\n",
    "    return num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['음절 개수'] = data['본문'].apply(count_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (4) 파생 변수 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평균 문장 길이\n",
    "data['평균 문장 길이'] = data['단어 개수'] / data['바른 문장 개수']\n",
    "\n",
    "# 평균 단어 길이\n",
    "data['평균 단어 길이'] = data['음절 개수'] / data['음절 개수']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (5) 명사 난이도 파악 후 어려운 어휘 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------# 바른으로 품사 태깅\n",
    "desired_pos_tags = ['NNG']\n",
    "\n",
    "# 시작 시간 기록\n",
    "start_time = time.time()\n",
    "\n",
    "# 값을 저장할 리스트\n",
    "score_column = []\n",
    "\n",
    "# '본문'에 대한 처리 시간과 진행 상황을 모니터링\n",
    "for i, line in enumerate(data['본문'], start=1):\n",
    "    \n",
    "    score_list = []\n",
    "    \n",
    "    res = my_tagger.tags([line])\n",
    "\n",
    "    # 'ma' 리스트에 있는 형태소의 총 빈도수 합산\n",
    "    total_usage = 0\n",
    "    \n",
    "    filtered_result = [(word, pos) for word, pos in res.pos() if pos in desired_pos_tags]\n",
    "    tokens = [word for word, _ in filtered_result]\n",
    "    \n",
    "    for token in tokens:\n",
    "        daily_freq = daily_usage.loc[daily_usage['corpus'] == token, 'frequency'].mean()\n",
    "        wave_freq = wave_corp.loc[wave_corp['단어'] == token, '총 빈도수'].mean()\n",
    "        if np.isnan(daily_freq):\n",
    "            score = wave_freq / 1\n",
    "        else:\n",
    "            score = wave_freq / daily_freq\n",
    "        \n",
    "        score_tuple = (token, score) # 토큰과 score를 tuple 형태로 저장하고\n",
    "        score_list.append(score_tuple) # 리스트에 하나씩 저장\n",
    "    \n",
    "    score_column.append(score_list) # for문 다 돌아간 리스트들을 다시 최종 리스트 내에 저장\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Score Calculating Processed {i} samples. Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# 데이터에 추가\n",
    "data['점수_tuple'] = score_column\n",
    "\n",
    "#------------------------------# 단어만 뽑아내기\n",
    "total_word_list = []\n",
    "\n",
    "for i in range(len(data)):\n",
    "    line = data.iloc[i, 2]\n",
    "    word_list = []\n",
    "    for (word, num) in line:\n",
    "        word_list.append(word)\n",
    "    total_word_list.append(word_list)\n",
    "    \n",
    "#------------------------------# Counter 함수로 빈도수(count)를 반영해보자!\n",
    "total_result_list = []\n",
    "\n",
    "for l in total_word_list:\n",
    "    asd = Counter(l)\n",
    "    word_frequency_pairs = list(asd.items())\n",
    "    result_list = []\n",
    "    result_list.extend(word_frequency_pairs)\n",
    "    total_result_list.append(result_list)\n",
    "    \n",
    "# 데이터에 추가\n",
    "data['counter'] = total_result_list\n",
    "\n",
    "#------------------------------# 점수와 빈도수를 곱하기\n",
    "result_list = []\n",
    "\n",
    "for i in range(len(data)): # 데이터셋의 모든 행에 대해\n",
    "    list1 = data.iloc[i, 2] # '점수_tuple' 열에 접근\n",
    "    list2 = data.iloc[i, 3] # 'counter' 열에 접근\n",
    "    \n",
    "    new_tuple_list = []\n",
    "\n",
    "    # 첫번째 리스트를 기준으로 루프\n",
    "    for tup1 in list1:        \n",
    "        # 튜플에서 단어 추출\n",
    "        word = tup1[0]\n",
    "\n",
    "        # 두번째 리스트에서 해당 단어에 대한 튜플 찾기\n",
    "        matching_tup2 = next((tup for tup in list2 if tup[0] == word), None)\n",
    "\n",
    "        # 만약에 찾은 경우\n",
    "        if matching_tup2 is not None:\n",
    "            # nan과 숫자를 곱해 새로운 튜플 생성\n",
    "            new_tuple = (word, matching_tup2[1] * tup1[1])\n",
    "            new_tuple_list.append(new_tuple)\n",
    "            \n",
    "    # 결과 리스트에 새 튜플리스트들을 축적!\n",
    "    result_list.append(new_tuple_list)\n",
    "    \n",
    "# 데이터에 추가\n",
    "data['단어난이도'] = result_list\n",
    "\n",
    "#------------------------------# (단어, 빈도수) 쌍에 대해서 nan 제거, 중복 제거, 빈도수 내림차순 정렬 그리고 불용어 제거\n",
    "stopwords = ['요구', '특파원', '참석자', '희생자', '기자', '지난해', '양국', '갑', '을', '중대형', '승용차', '이듬해', '핸드볼',\n",
    "             '국가', '연합뉴스', '당국', '지난해', '기업', '상승세', '닷새', '누리집', '꼴찌', '사망자', '이날', '대통령', '지역',\n",
    "             '시인', '메시지', '센터', '시', '의료원', '붕괴', '기자실', '보고서', '소폭', '라이벌', '노조', '내년도', '견제', '앵커',\n",
    "             '논설위원', '수락', '리서치', '타임스', '무죄', '뉴시스', '도', '조사', '상당수', '지난달', '마다', '주', '가운데', '개방',\n",
    "            '연평균', '고속버스', '평균', '관계자', '고교', '연면적', '참가자', '당시']\n",
    "\n",
    "final_list = []\n",
    "\n",
    "for i in range(len(data)):\n",
    "    line = data.iloc[i, 4]  # '단어난이도' 열에 접근\n",
    "    line_without_nan = [(word, num) for word, num in line if not pd.isna(num)]  # tuple 내에 nan 있으면 제거\n",
    "    sorted_result = sorted(line_without_nan, key=lambda x: x[1], reverse=True)  # 최종 난이도 기준으로 내림차순 정렬\n",
    "\n",
    "    unique_values = set()\n",
    "    unique_result = [(word, num) for word, num in sorted_result if word not in unique_values and not unique_values.add(word)]  # 중복 제거\n",
    "\n",
    "    # Remove stopwords\n",
    "    unique_result = [(word, num) for word, num in unique_result if word not in stopwords]\n",
    "\n",
    "    final_list.append(unique_result)\n",
    "\n",
    "# 샘플데이터에 추가 및 확인\n",
    "data['단어난이도'] = final_list\n",
    "\n",
    "#------------------------------# 최종난이도만 뽑아내기\n",
    "num_list = []\n",
    "\n",
    "for i in range(len(data)):\n",
    "    line = data.iloc[i, 4] # '단어난이도' 열에 접근\n",
    "    for (_, num) in line:\n",
    "        num_list.append(num)\n",
    "\n",
    "#------------------------------# 최종난이도 상위 6개만 추출\n",
    "top6_list = []\n",
    "\n",
    "for i in range(len(data)):\n",
    "    line = data.iloc[i, 4]\n",
    "    lbyl_list = []\n",
    "    for (word, num) in line:\n",
    "        lbyl_list.append(word)\n",
    "        if len(lbyl_list) == 6:\n",
    "            break\n",
    "    top6_list.append(lbyl_list)\n",
    "\n",
    "# 데이터에 추가\n",
    "data['top6'] = top6_list\n",
    "\n",
    "#------------------------------# 단어장 제공 단어\n",
    "\n",
    "\n",
    "#------------------------------# 최종난이도 상위 6개만 추출 새로운 리스트를 담을 변수\n",
    "final_result = []\n",
    "\n",
    "for i in range(len(data)): # 데이터셋의 모든 행에 대해\n",
    "    list1 = data.iloc[i, 4] # '단어난이도' 열에 접근\n",
    "                           \n",
    "    단어난이도_list = []\n",
    "\n",
    "    for tup1 in list1:        \n",
    "        # 튜플에서 숫자 추출\n",
    "        단어난이도 = tup1[1]\n",
    "        단어난이도_list.append(단어난이도)\n",
    "            \n",
    "    # 결과 리스트에 새 튜플리스트들을 축적!\n",
    "    단어난이도_sum = sum(단어난이도_list)/len(단어난이도_list)\n",
    "    final_result.append(단어난이도_sum)\n",
    "\n",
    "# 데이터에 추가\n",
    "data['명사난이도 평균 점수'] = final_result\n",
    "\n",
    "# 전체 실행 시간 출력\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Final Score Calculated elapsed time: {total_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 지정\n",
    "stopwords = ['요구', '특파원', '참석자', '희생자', '기자', '지난해', '양국', '갑', '을', '중대형', '승용차', '이듬해', '핸드볼',\n",
    "             '국가', '연합뉴스', '당국', '지난해', '기업', '상승세', '닷새', '누리집', '꼴찌', '사망자', '이날', '대통령', '지역',\n",
    "             '시인', '메시지', '센터', '시', '의료원', '붕괴', '기자실', '보고서', '소폭', '라이벌', '노조', '내년도', '견제', '앵커',\n",
    "             '논설위원', '수락', '리서치', '타임스', '무죄', '뉴시스', '도', '조사', '상당수', '지난달', '마다', '주']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySang",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
