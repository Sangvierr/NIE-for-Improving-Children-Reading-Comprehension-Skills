{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 데이터 크롤링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 빅카인즈에서 4대 신문사의 4개의 각 카테고리에서 5개의 키워드를 사용해서 10년치 신문 기사를 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common import NoSuchElementException, StaleElementReferenceException\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigkindsCrawler:\n",
    "    def __init__(self, path, year):\n",
    "        self.df = pd.read_csv(path, encoding=\"utf-8-sig\")\n",
    "        self.year = year\n",
    "        # 각 월 별 날짜 수 (Hard-coded) & 각 월\n",
    "        self.months = [\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\"]\n",
    "        self.days = [\"31\", \"28\", \"31\", \"30\", \"31\", \"30\", \"31\", \"31\", \"30\", \"31\", \"30\", \"31\"]\n",
    "        self.options = webdriver.ChromeOptions()\n",
    "\n",
    "    # set the WebDriver options\n",
    "    def set_driver_options(self):\n",
    "        self.options.add_argument('--window-size=1920,1080')\n",
    "        self.options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "        # set User-Agent for preventing access blocked\n",
    "        self.options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64)\" +\n",
    "                                  \"AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36\")\n",
    "        # prevent webdriver from closing immediately\n",
    "        self.options.add_experimental_option(\"detach\", True)\n",
    "        # 크롬 브라우저가 직접적으로 열리지 않도록 설정\n",
    "        self.options.add_argument('--headless')\n",
    "        # 불필요한 이미지 로딩 없앰 (시간 단축)\n",
    "        self.options.add_argument('--disable-logging')\n",
    "        self.options.add_argument('--disable-images')\n",
    "\n",
    "    # csv 파일 필요: publisher, keyword를 csv로 먹임\n",
    "    # category: 통합 분류 (li: 정치=1, 경제=2, 사회=3, 국제=5)\n",
    "    def executor(self, publisher, m, category, keyword):\n",
    "        res = []\n",
    "\n",
    "        start_day = self.year + \"-\" + self.months[m] + \"-\" + \"01\"\n",
    "        end_day = self.year + \"-\" + self.months[m] + \"-\" + self.days[m]\n",
    "\n",
    "        # webdriver 생성\n",
    "        driver = webdriver.Chrome(options=self.options)\n",
    "        driver.get(\"https://www.bigkinds.or.kr/v2/news/index.do\")\n",
    "\n",
    "        # 언론사 클릭\n",
    "        pub = self.transform_publisher(publisher)\n",
    "        driver.find_element(By.XPATH, f\"//*[@id='category_provider_list']/li[{pub}]/span/label\").click()\n",
    "        time.sleep(0.5)\n",
    "\n",
    "        # 기간 클릭 (배너)\n",
    "        driver.find_element(By.XPATH, \"//*[@id='collapse-step-1-body']/div[3]/div/div[1]/div[1]/a\").click()\n",
    "        # 기간 클릭 (1개월)\n",
    "        driver.find_element(By.XPATH, \"//*[@id='srch-tab1']/div/div[1]/span[3]/label\").click()\n",
    "\n",
    "        # 시작 날짜 클릭\n",
    "        driver.find_element(By.XPATH, \"//*[@id='srch-tab1']/div/div[2]/div/div[1]/img\").click()\n",
    "        start = driver.find_element(By.XPATH, \"//*[@id='search-begin-date']\")\n",
    "        start.send_keys(Keys.CONTROL, 'a')\n",
    "        start.send_keys(start_day)\n",
    "\n",
    "        # 종료 날짜 클릭\n",
    "        driver.find_element(By.XPATH, \"//*[@id='srch-tab1']/div/div[2]/div/div[3]/img\").click()\n",
    "        end = driver.find_element(By.XPATH, \"//*[@id='search-end-date']\")\n",
    "        end.send_keys(Keys.CONTROL, 'a')\n",
    "        end.send_keys(end_day)\n",
    "        time.sleep(0.5)\n",
    "\n",
    "        # 통합 분류 클릭 (배너)\n",
    "        # 그냥 클릭하면 페이지 로딩 시간 때문에 오류가 날 수 있어서 webdriver 기다림\n",
    "        element = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, \"//*[@id='collapse-step-1-body']/div[3]/div/div[2]/div[1]/a\"))\n",
    "        )\n",
    "        element.click()\n",
    "\n",
    "        # 통합 분류 (li: 정치=1, 경제=2, 사회=3, 국제=5)\n",
    "        driver.find_element(By.XPATH, f\"//*[@id='srch-tab3']/ul/li[{category}]/div/span[4]\").click()\n",
    "        time.sleep(1)\n",
    "\n",
    "        # 키워드 입력: 오류 나지 않게 한 글자씩 입력함\n",
    "        keyword_input = driver.find_element(By.XPATH, \"//*[@id='total-search-key']\")\n",
    "        for k in keyword:\n",
    "            keyword_input.send_keys(k)\n",
    "            time.sleep(0.2)\n",
    "        keyword_input.send_keys(Keys.RETURN)\n",
    "        time.sleep(1)\n",
    "\n",
    "        # 정확도순\n",
    "        driver.find_element(By.XPATH, \"//*[@id='select1']/option[2]\").click()\n",
    "        time.sleep(1)\n",
    "\n",
    "        try:\n",
    "            # 맨 위의 기사 클릭\n",
    "            driver.find_element(By.XPATH, \"//*[@id='news-results']/div[1]/div/div[2]\").click()\n",
    "            time.sleep(1)\n",
    "\n",
    "            # \"일자\", \"언론사\", \"제목\", \"URL\", \"본문\"\n",
    "            try:\n",
    "                date = driver.find_element(By.XPATH,\n",
    "                                           \"//*[@id='news-detail-modal']/div/div/div[1]/div/div[1]/div[1]/ul/li[1]\").text\n",
    "            except NoSuchElementException:\n",
    "                date = \"N/A\"\n",
    "\n",
    "            title = driver.find_element(By.XPATH, \"//*[@id='news-detail-modal']/div/div/div[1]/div/div[1]/h1\").text\n",
    "\n",
    "            # URL 오류 처리\n",
    "            href_button = driver.find_element(By.XPATH,\n",
    "                                              \"//*[@id='news-detail-modal']/div/div/div[1]/div/div[1]/div[2]/div[1]/button[1]\")\n",
    "\n",
    "            if href_button.text == \"기사원문\":\n",
    "                href = href_button.get_attribute(\"onclick\")\n",
    "\n",
    "                # ?가 포함되었을 경우, 쿼리 문자열이므로 뒤의 문자열은 삭제\n",
    "                try:\n",
    "                    url = re.search(r'https?://[^?]+', href).group()\n",
    "                except AttributeError:\n",
    "                    # URL이 매치되지 않는 경우, 예외 처리를 통해 http 이후의 문자열만 저장\n",
    "                    url = re.search(r'https?://+', href).group()\n",
    "            else:\n",
    "                url = \"N/A\"\n",
    "\n",
    "            paper = driver.find_element(By.XPATH, \"//*[@id='news-detail-modal']/div/div/div[1]/div/div[2]\")\n",
    "            main_text = paper.text\n",
    "        except (StaleElementReferenceException, NoSuchElementException):\n",
    "            date = \"N/A\"\n",
    "            publisher = \"N/A\"\n",
    "            title = \"N/A\"\n",
    "            url = \"N/A\"\n",
    "            main_text = \"N/A\"\n",
    "\n",
    "        res.append(date)\n",
    "        res.append(publisher)\n",
    "        res.append(title)\n",
    "        res.append(url)\n",
    "        res.append(main_text)\n",
    "\n",
    "        driver.quit()\n",
    "\n",
    "        print(res)\n",
    "        return res\n",
    "\n",
    "    # 월 단위별로 크롤링\n",
    "    def crawling(self, MONTH):\n",
    "        crawled_df = pd.DataFrame(columns=[\"일자\", \"언론사\", \"제목\", \"URL\", \"본문\"])\n",
    "\n",
    "        s_index = (MONTH - 1) * 16\n",
    "        size = 16\n",
    "        publishers = self.df.loc[s_index:s_index + size, \"언론사\"]\n",
    "\n",
    "        categories = self.df.loc[s_index:s_index + size, \"카테고리\"]\n",
    "\n",
    "        total_time = 0\n",
    "\n",
    "        for i in range(s_index, s_index + size):\n",
    "            rank_str = self.df.loc[i, \"top-10 키워드\"]\n",
    "            rank_str = rank_str.replace(\"'\", '\"')\n",
    "\n",
    "            # JSON 문자열을 파이썬 리스트로 변환\n",
    "            data_list = json.loads(rank_str)\n",
    "            data_list = data_list[:5]\n",
    "\n",
    "            keywords = [item['name'] for item in data_list]\n",
    "\n",
    "            publisher = publishers[i]\n",
    "            category = categories[i] // 1000000\n",
    "\n",
    "            # 변수 체크용\n",
    "            print(f\"CSV 행 = {i}\")\n",
    "            print(f\"언론사 = {publisher}\")\n",
    "            print(f\"카테고리 = {category}\")\n",
    "\n",
    "            for j, keyword in enumerate(keywords):\n",
    "                s = time.time()\n",
    "                print(\n",
    "                    f\"{'|||||  * Process: ' + str(MONTH) + 'th month ' + str(i * 5 + j + 1) + 'th/' + '960th *  |||||':^50}\")\n",
    "                crawled_df.loc[i * 5 + j, :] = self.executor(publisher, MONTH - 1, category, keyword)\n",
    "                e = time.time()\n",
    "                total_time += round(e - s, 2)\n",
    "                print(f\"Elapsed time: {total_time:.2f}\")\n",
    "                time.sleep(1)\n",
    "\n",
    "        return crawled_df\n",
    "\n",
    "    def transform_publisher(self, p):\n",
    "        pub = 0\n",
    "        if p == \"경향신문\":\n",
    "            pub = 1\n",
    "        elif p == \"동아일보\":\n",
    "            pub = 4\n",
    "        elif p == \"조선일보\":\n",
    "            pub = 8\n",
    "        elif p == \"중앙일보\":\n",
    "            pub = 9\n",
    "        elif p == \"한겨레\":\n",
    "            pub = 10\n",
    "\n",
    "        return pub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2015년 크롤링\n",
    "YEAR = \"2015\"\n",
    "\n",
    "# 1 ~ 12월까지 크롤링하고, 각 월 별로 데이터 프레임을 만듭니다.\n",
    "for Month in range(6, 7):\n",
    "    crawler = BigkindsCrawler(f\"topkeywords_{YEAR}.csv\", YEAR)\n",
    "    crawler.set_driver_options()  # 옵션 세팅\n",
    "\n",
    "    dataframe = crawler.crawling(Month)\n",
    "    dataframe.to_csv(f\"{YEAR}_{Month}.csv\", encoding=\"utf-8-sig\")\n",
    "\n",
    "    print(\"Waiting..\")\n",
    "    time.sleep(5)\n",
    "    # 메모리 삭제 후 재할당을 위한 코드\n",
    "    del crawler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('C:/Users/user/PD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터를 언론사 별로 분리\n",
    "def seperate_by_press(df):\n",
    "    JoonAng_news = df[df['언론사'] == '중앙일보']\n",
    "    DongA_news = df[df['언론사'] == '동아일보']\n",
    "    Hani_news = df[df['언론사'] == '한겨레']\n",
    "    Kyung_news = df[df['언론사'] == '경향신문']\n",
    "    return JoonAng_news, DongA_news, Hani_news, Kyung_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 신문 기사 텍스트 전처리\n",
    "def cleaning_text(text):\n",
    "    # text가 문자열이 아니면 그대로 반환\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    # (/br)로 나오는 경우 제거\n",
    "    text = text.replace('(/br)', '')\n",
    "    # 특수 문자 지정 후 제거\n",
    "    text = re.sub(r'[☞▶◆#⊙※△▽▼□■◇◎☎○]+', '', text, flags=re.UNICODE)\n",
    "    text = re.sub(r'〃', '', text)  \n",
    "    # 한자 및 일본어 제거\n",
    "    text = re.sub(r'[\\p{Script=Hiragana}\\p{Script=Katakana}\\p{Script=Han}]+', '', text, flags=re.UNICODE)\n",
    "    # 이메일 주소 제거\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    # 사이트 주소 제거(www. 으로 시작하고 .kr로 끝나는 경우)\n",
    "    text = re.sub(r'www\\..+\\.kr', '', text)\n",
    "    \n",
    "    # 대괄호로 둘러싸인 내용을 삭제 (10글자 미만인 경우는 삭제, 10글자 이상인 경우는 유지)\n",
    "    text = re.sub(r'\\[([^\\]]{1,9})\\]', '', text)\n",
    "    text = re.sub(r'\\[([^\\]]{10,})\\]', r'\\1', text)\n",
    "    # \"<...>\"로 둘러싸인 내용을 삭제\n",
    "    text = re.sub(r'<[^>]*>', '', text)\n",
    "    # 소괄호 안에 아무런 내용도 없으면 삭제\n",
    "    text = re.sub(r'\\(\\s*\\)', '', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def preprocess_news(df, column_name = '본문'):\n",
    "    df[column_name] = df[column_name].apply(cleaning_text)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경로 설정\n",
    "data_dir = 'C:/Users/user/PD/input/2014'  # 데이터 디렉토리 경로\n",
    "output_dir = 'C:/Users/user/PD/output/2014'  # 출력 디렉토리 경로\n",
    "os.makedirs(output_dir, exist_ok=True)  # 출력 디렉토리 생성 (없으면)\n",
    "\n",
    "# 년도와 월에 대한 범위 설정\n",
    "start_year = 2014\n",
    "end_year = 2014\n",
    "months = range(1, 13)\n",
    "\n",
    "# 월별로 데이터 처리\n",
    "for year in range(start_year, end_year + 1):\n",
    "    for month in months:\n",
    "        # 데이터 파일명 생성\n",
    "        file_name = f'{year}_{month}.csv'\n",
    "        data_path = os.path.join(data_dir, file_name)\n",
    "        \n",
    "        # 데이터 불러오기\n",
    "        news = pd.read_csv(data_path, encoding='utf-8')\n",
    "        \n",
    "        # 전처리 수행\n",
    "        news = preprocess_news(news)\n",
    "        \n",
    "        # 전처리된 데이터 저장\n",
    "        output_file_name = f'processed_{file_name}'\n",
    "        output_path = os.path.join(output_dir, output_file_name)\n",
    "        news.to_csv(output_path, encoding='utf-8-sig', index=False)\n",
    "        print(f'{file_name} 전처리 및 저장 완료')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySang",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
